#
# Copyright (c) 2016. Universidad Politecnica de Madrid
#
# @author Badenes Olmedo, Carlos <cbadenes@fi.upm.es>
#
#

librairy.computing.home=librairy

librairy.computing.memory = -1
librairy.computing.cluster = local[*]
librairy.computing.cores = 120

# Max size of task in parallel processing (in KB)
librairy.computing.task.size = 200

# Spark
librairy.computing.spark.path = /var/lib/spark
librairy.computing.spark.package = spark-1.6.2-bin-hadoop2.6

# cache
# MEMORY_ONLY           : Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.
# MEMORY_AND_DISK 	    : Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.
# MEMORY_ONLY_SER       : Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.
# MEMORY_AND_DISK_SER   : Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.
# DISK_ONLY             : Store the RDD partitions only on disk.
librairy.computing.cache = MEMORY_AND_DISK_SER


# FileSystem
librairy.computing.fs = local